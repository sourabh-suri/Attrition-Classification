{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom openpyxl import load_workbook\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import svm, tree, linear_model, neighbors\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nimport os\n\n\n# Reading Source Files and getting their paths.\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint(os.listdir(\"../input\"))\nsourcefile = pd.read_csv('../input/ee-769-assignment1/train.csv')\nsourcefile_test = pd.read_csv('../input/ee-769-assignment1/test.csv')\n\n# Make a copy of the original sourcefile\nTrain_Data = sourcefile.copy()\nTest_Data = sourcefile_test.copy()\nID = Train_Data['ID'].copy()\nID_test = []\nID_test=Test_Data['ID'].copy()\n\n# Encoding\n    # Training Data\nEncode_obj = LabelEncoder()\nfor col in Train_Data.columns[1:]:\n    if Train_Data[col].dtype == 'object': # Label Encoding for atmost two levels\n        if len(list(Train_Data[col].unique())) <= 2: \n            Encode_obj.fit(Train_Data[col])\n            Train_Data[col] = Encode_obj.transform(Train_Data[col])\n    # Test Data\nEncode_test = LabelEncoder()\nfor col in Test_Data.columns[1:]:\n    if Test_Data[col].dtype == 'object':\n        if len(list(Test_Data[col].unique())) <= 2:\n            Encode_test.fit(Test_Data[col])\n            Test_Data[col] = Encode_test.transform(Test_Data[col])\n    # Converting rest of categorical variable to dummy\nTrain_Data = pd.get_dummies(Train_Data, drop_first=True)\nTest_Data = pd.get_dummies(Test_Data, drop_first=True)\nprint(Train_Data.shape)\n\n# Scaling\nscaler = MinMaxScaler(feature_range=(0, 1))\n    # Train\nData_col = list(Train_Data.columns)\nData_col.remove('Attrition')\nfor col in Data_col:\n    Train_Data[col] = Train_Data[col].astype(float)\n    Train_Data[[col]] = scaler.fit_transform(Train_Data[[col]])\nTrain_Data['Attrition'] = pd.to_numeric(Train_Data['Attrition'], downcast='float')\n    # Test\nTest_Col = list(Test_Data.columns)\nfor col in Test_Col:\n    Test_Data[col] = Test_Data[col].astype(float)\n    Test_Data[[col]] = scaler.fit_transform(Test_Data[[col]])\n\n# Copying Target and ID\ntarget = Train_Data['Attrition'].copy()\n\n# Removing Target and redundant features\n\nTrain_Data.drop(['Attrition', 'EmployeeCount', 'EmployeeNumber','ID','MonthlyRate','HourlyRate'], axis=1, inplace=True)\nTest_Data.drop(['EmployeeCount', 'EmployeeNumber','ID','MonthlyRate','HourlyRate'], axis=1, inplace=True)\n\n#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1,max_iter=500, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0)\nprint(\"Logistic Regression : \",cross_val_score(logmodel, Train_Data, target, cv=5, scoring='accuracy').mean())\nlogmodel.fit(Train_Data,target)\npredictions_LR = logmodel.predict(Test_Data)\n\n#KNeighbors Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='brute',leaf_size=30, p=2, metric='minkowski')                       \nprint(\"KNeighborsClassifier : \",cross_val_score(knn, Train_Data, target, cv=20, scoring='accuracy').mean())\nknn.fit(Train_Data,target)\npredictions_KNN = knn.predict(Test_Data)\n\n#Support Vector Machine\nSVM=SVC(C=1.0, kernel='linear', degree=1, gamma='auto', \n        coef0=0.0, shrinking=True, probability=False, \n        tol=0.001, cache_size=200, class_weight=None, \n        verbose=False, max_iter=-1, decision_function_shape='ovo')\n#SVM=svm.SVC(kernel='linear', C=1)\nprint(\"Support Vector Machine : \",cross_val_score(SVM, Train_Data, target, cv=150, scoring='accuracy').mean())\nSVM.fit(Train_Data,target)\npredictions_SVM = SVM.predict(Test_Data)\n\n# Random Forest\nRF = RandomForestClassifier(max_depth=50, random_state=0)\nprint(\"Random Forest : \",cross_val_score(RF, Train_Data, target, cv=10, scoring='accuracy').mean())\nRF.fit(Train_Data,target)\npredictions_RF = RF.predict(Test_Data)\n\n# Decision Tree Classifier\nDTC = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=50, min_samples_split=2, \n                             min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, \n                             random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n                             min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\nprint(\"Decision Tree Classifier : \",cross_val_score(DTC, Train_Data, target, cv=10, scoring='accuracy').mean())\nDTC.fit(Train_Data,target)\npredictions_DTC = DTC.predict(Test_Data)\n\n# Neural Network- Multi-layer Perceptron Classifier\nMLP = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, \n                    batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, \n                    max_iter=2500, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, \n                    momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, \n                    beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n\nprint(\"Multi-layer Perceptron Classifier : \",cross_val_score(MLP, Train_Data, target, cv=50, scoring='accuracy').mean())\nMLP.fit(Train_Data,target)\npredictions_MLP = MLP.predict(Test_Data)\n\n\n# Gaussian Naive Bayes \nNB = GaussianNB()\nprint(\"Gaussian Naive Bayes : \",cross_val_score(NB, Train_Data, target, cv=10, scoring='accuracy').mean())\nNB.fit(Train_Data,target)\npredictions_NB = NB.predict(Test_Data)\n\n\n\n#Output Generation\noutput=[]\noutput=np.asarray(output)\nID_test=np.asarray(ID_test)\n\n# Take the prediction value based on Classifier...\n#predictions=np.asarray(predictions_LR)\n#predictions=np.asarray(predictions_KNN)\npredictions=np.asarray(predictions_SVM)\n#predictions=np.asarray(predictions_RF)\n#predictions=np.asarray(predictions_DTC)\n#predictions=np.asarray(predictions_MLP)\n#predictions=np.asarray(predictions_NB)\n\ntemp= np.column_stack((ID_test, predictions))\noutput = np.append(output,temp)\noutput = np.reshape(output,(ID_test.shape[0],2))\nnp.savetxt('A_2_Output.csv',output, fmt=\"%d\", header='ID,Attrition',delimiter=\",\",comments='')\n\n\n","execution_count":null,"outputs":[{"output_type":"stream","text":"/kaggle/input/ee-769-assignment1/test.csv\n/kaggle/input/ee-769-assignment1/sample_submission.csv\n/kaggle/input/ee-769-assignment1/train.csv\n['ee-769-assignment1']\n(1028, 48)\nLogistic Regression :  0.8686810324413925\nKNeighborsClassifier :  0.8483031674208146\nSupport Vector Machine :  0.8745714285714286\nRandom Forest :  0.8462592804111935\nDecision Tree Classifier :  0.7771749476489624\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}